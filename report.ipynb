{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing dataset metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "ds_meta_files = {\n",
    "    'bounding_boxes': ['image_guid', 'lower_x', 'lower_y', 'upper_x', 'upper_y'],\n",
    "    'classes': ['id', 'name'],\n",
    "    'hierarchy': ['id', 'parent_id'],\n",
    "    'image_class_labels': ['image_guid', 'class_id'],\n",
    "    'images': ['image_guid', 'relative_path'],\n",
    "    'photographers': ['image_guid', 'name'],\n",
    "    'sizes': ['image_guid', 'width', 'height']\n",
    "}\n",
    "ds_meta = {}\n",
    "\n",
    "for ds_meta_file in ds_meta_files:\n",
    "    with open ('data/%s.txt' % ds_meta_file, 'r' ) as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    for i in range(0, len(ds_meta_files[ds_meta_file]) - 1):\n",
    "        content = re.sub('\\ (.*)(\\n|\\Z)', r'|\\1\\2', content, flags = re.M)\n",
    "    \n",
    "    with open ('data/%s.csv' % ds_meta_file, 'w') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    ds_meta[ds_meta_file] = pd.read_csv('data/%s.csv' % ds_meta_file, header=None, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantity of bounding_boxes: 48562\n",
      "Quantity of classes: 1011\n",
      "Quantity of hierarchy: 1010\n",
      "Quantity of image_class_labels: 48562\n",
      "Quantity of images: 48562\n",
      "Quantity of photographers: 48562\n",
      "Quantity of sizes: 48562\n"
     ]
    }
   ],
   "source": [
    "for ds_meta_item in ds_meta:\n",
    "    print('Quantity of %s: %s' % (ds_meta_item, ds_meta[ds_meta_item].count()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# BASE               SET_A\n",
    "# + Gabor filter  => SET_A_GF               (del)\n",
    "# + Bounding box  => SET_A_GF_BB            (del)\n",
    "# + TVT splitting => SET_A_GF_BB_train\n",
    "#                    SET_A_GF_BB_validation\n",
    "#                    SET_A_GF_BB_test\n",
    "\n",
    "def apply_gabor_filter(in_path, out_path, class_subdirs=True):\n",
    "    g_kernel = cv2.getGaborKernel((21, 21), 8.0, np.pi/4, 10.0, 0.5, 0, ktype=cv2.CV_32F)\n",
    "    \n",
    "    subdirs = [''] if not class_subdirs else os.listdir(in_path)\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        in_subdir_p = os.path.join(*(in_path, subdir))\n",
    "        out_subdir_p = os.path.join(*(out_path, subdir))\n",
    "        os.makedirs(out_subdir_p, exist_ok=True)\n",
    "        \n",
    "        for item in os.listdir(in_subdir_p):\n",
    "            img = cv2.imread(os.path.join(*(in_subdir_p, item)))\n",
    "            img_f = cv2.filter2D(img, cv2.CV_8UC3, g_kernel)\n",
    "            cv2.imwrite(os.path.join(*(out_subdir_p, item)), img_f)\n",
    "            \n",
    "def apply_bounding_box(in_path, out_path, class_subdirs=True):\n",
    "    # TODO implement\n",
    "    pass\n",
    "\n",
    "def apply_tvt_split(path, train=0.7, test=0.3, validation=0.0, class_subdirs=True):\n",
    "    dir_train_p = '%s_train' % path\n",
    "    dir_validation_p = '%s_validation' % path\n",
    "    dir_test_p = '%s_test' % path\n",
    "    \n",
    "    if os.path.exists(dir_train_p) and os.path.isdir(dir_train_p):\n",
    "        shutil.rmtree(dir_train_p)\n",
    "    if os.path.exists(dir_validation_p) and os.path.isdir(dir_validation_p):\n",
    "        shutil.rmtree(dir_validation_p)\n",
    "    if os.path.exists(dir_test_p) and os.path.isdir(dir_test_p):\n",
    "        shutil.rmtree(dir_test_p)\n",
    "        \n",
    "    subdirs = [''] if not class_subdirs else os.listdir(path)\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        subdir_p = os.path.join(*(path, subdir))\n",
    "        subdir_list = os.listdir(subdir_p)\n",
    "        \n",
    "        random.shuffle(subdir_list)\n",
    "        \n",
    "        subdir_list_len = len(subdir_list)\n",
    "        subdir_list_train_thld = int(subdir_list_len * train)\n",
    "        subdir_list_validation_thld = int(subdir_list_len * validation) + subdir_list_train_thld\n",
    "        \n",
    "        subdir_list_train = subdir_list[:subdir_list_train_thld]\n",
    "        subdir_list_validation = subdir_list[subdir_list_train_thld:subdir_list_validation_thld]\n",
    "        subdir_list_test = subdir_list[subdir_list_validation_thld:]\n",
    "        \n",
    "        # prepare train part\n",
    "        subdir_train_p = os.path.join(*(dir_train_p, subdir))\n",
    "        os.makedirs(subdir_train_p, exist_ok=True)\n",
    "        for subdir_list_train_item in subdir_list_train:\n",
    "            src = os.path.join(subdir_p, subdir_list_train_item)\n",
    "            dest = os.path.join(subdir_train_p, subdir_list_train_item)\n",
    "            shutil.copyfile(src, dest)\n",
    "            \n",
    "        # prepare validation part\n",
    "        if len(subdir_list_validation) > 0:\n",
    "            subdir_validation_p = os.path.join(*(dir_validation_p, subdir))\n",
    "            os.makedirs(subdir_validation_p, exist_ok=True)\n",
    "            for subdir_list_validation_item in subdir_list_validation:\n",
    "                src = os.path.join(subdir_p, subdir_list_validation_item)\n",
    "                dest = os.path.join(subdir_validation_p, subdir_list_validation_item)\n",
    "                shutil.copyfile(src, dest)\n",
    "            \n",
    "        # prepare test part\n",
    "        subdir_test_p = os.path.join(*(dir_test_p, subdir))\n",
    "        os.makedirs(subdir_test_p, exist_ok=True)\n",
    "        for subdir_list_test_item in subdir_list_test:\n",
    "            src = os.path.join(subdir_p, subdir_list_test_item)\n",
    "            dest = os.path.join(subdir_test_p, subdir_list_test_item)\n",
    "            shutil.copyfile(src, dest)\n",
    "\n",
    "apply_gabor_filter('data/SET_A', 'data/SET_A_GF')\n",
    "apply_tvt_split('data/SET_A_GF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building simple perceptron with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sitkom/Code/nn-nbirds/venv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2103 images belonging to 50 classes.\n",
      "Epoch 1/50\n",
      " 96/125 [======================>.......] - ETA: 15s - loss: 9.6221 - acc: 0.0234"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_labels = os.listdir('data/SET_A_GF_train')\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'data/SET_A_GF_train',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=batch_size,\n",
    "    classes=train_labels)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(150, 150, 3))) \n",
    "model.add(Dense(64))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(len(train_labels)))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "* BSIF http://www.ee.oulu.fi/~jkannala/bsif/bsif.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
