{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Mntr59DSahw"
   },
   "source": [
    "**Michał Sitko, Michał Chmielowiec, Andrzej Dawidziuk**\n",
    "# Neural Networks (SNR) 2018L - Final Report\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aW2_p_xSSah0"
   },
   "source": [
    "We decided to use [Google Collaboratory](https://colab.research.google.com) as a main platform for project development. So called Colab was developed by Google internally from year 2014 as a tool dedicated for machine learning reaserch. Now it is publicly available as a side Google Drive service.\n",
    "\n",
    "Colab is an online version of Jupyter Notebook - a web application that allows its users to create and share documents that contain live code, equations, visualizations and narrative text. Usually Jupyter Notebooks are hosted locally on researchers' machines. Very often these machines are not equipped with processing units suited for complex calculations. Locally prepared Jupyter reports are printed as PDF documents and then statically distributed.\n",
    "\n",
    "Google Colab provides online version of Jupyter server with exposed by very convenient web interface. Each Google Drive account is allowed to use one Jupyter Notebook environment. Technically, environments are setup inside cloud hosted, UNIX type, Docker containers. The containers' virtual hard drives can expand to the maximum space available for particular account in its Google Drive service. Furthermore the container environment has access to pretty powerful computing resources: quad core CPU and Tesla K80 GPU. We should note here that those resources are shared by multiple users and thus its base performance may vary.\n",
    "\n",
    "*This report was prepared in Colab.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NSABI4opSah4"
   },
   "source": [
    "## 1. Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vt2_GzlrSah-"
   },
   "source": [
    "Let's ensure that the installed Python version is 2.7. It happens occasionally that containers are initialized with Python 3.6 kernels despite the fact that they the lower one is configured. In such a situation the hypervisor should be rebooted (i.e. by invoking a kill command)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UvFrie12LmWO",
    "outputId": "4ae003e0-8d9f-464c-afc7-21a2d6fde664"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.14 (default, Sep 23 2017, 22:06:14) \\n[GCC 7.2.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If python2 is not loaded kill the hypervisor\n",
    "# ! kill -9 -1\n",
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ggWs_k8nSaiQ"
   },
   "source": [
    "Thanks to Colab platform, GPU device is available!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "NKZL5ACblgQm",
    "outputId": "895e2d5b-028a-44f7-b5f3-faa0f7c7bb53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJLI2DYLSaig"
   },
   "source": [
    "By calling appropriate backend methods we can get some more specific info concerning processing devices that are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "AL_Ws1eo0CQU",
    "outputId": "003316c9-52f2-4da6-e437-4b0f877fa056"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 14074366061964417666, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 11287966516\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 13993350310858143274\n",
       " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j-N17s5RSaiw"
   },
   "source": [
    "Now we will clone the project repository and install required pip packages. Interactive shell can be accessed inline by prefixing commands with an exclamation mark (!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "Wb6yct3-XdMd",
    "outputId": "2b467064-8ca3-4725-ec9f-26a961b6e26a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Cloning into 'nn-nbirds'...\n",
      "remote: Counting objects: 310, done.\u001b[K\n",
      "remote: Total 310 (delta 0), reused 0 (delta 0), pack-reused 310\u001b[K\n",
      "Receiving objects: 100% (310/310), 548.66 MiB | 33.45 MiB/s, done.\n",
      "Resolving deltas: 100% (134/134), done.\n",
      "Checking out files: 100% (81/81), done.\n",
      "/content/nn-nbirds\n",
      "\u001b[31m  Could not find a version that satisfies the requirement ipython==6.3.1 (from -r requirements.txt (line 15)) (from versions: 0.10, 0.10.1, 0.10.2, 0.11, 0.12, 0.12.1, 0.13, 0.13.1, 0.13.2, 1.0.0, 1.1.0, 1.2.0, 1.2.1, 2.0.0, 2.1.0, 2.2.0, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 3.0.0, 3.1.0, 3.2.0, 3.2.1, 3.2.2, 3.2.3, 4.0.0b1, 4.0.0, 4.0.1, 4.0.2, 4.0.3, 4.1.0rc1, 4.1.0rc2, 4.1.0, 4.1.1, 4.1.2, 4.2.0, 4.2.1, 5.0.0b1, 5.0.0b2, 5.0.0b3, 5.0.0b4, 5.0.0rc1, 5.0.0, 5.1.0, 5.2.0, 5.2.1, 5.2.2, 5.3.0, 5.4.0, 5.4.1, 5.5.0, 5.6.0, 5.7.0)\u001b[0m\n",
      "\u001b[31mNo matching distribution found for ipython==6.3.1 (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[31m  Could not find a version that satisfies the requirement PIL (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for PIL\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "% cd ~\n",
    "\n",
    "# Remove the environment\n",
    "! if [ -d \"nn-nbirds\" ]; then rm -rf \"nn-nbirds\"; fi\n",
    "# ! pip freeze | xargs pip uninstall -y\n",
    "\n",
    "# Build the environment \n",
    "! git clone https://github.com/zacateras/nn-nbirds.git\n",
    "% cd ./nn-nbirds\n",
    "! pip install -r requirements.txt > pip.log\n",
    "# The commands below fix the issue with Keras @ Colab\n",
    "! pip install Pillow==4.0.0 -q\n",
    "! pip install PIL -q\n",
    "! pip install image -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9DN-B5nSajA"
   },
   "source": [
    "The whole **nbirds** database was stored in dropbox as a zip blob. We prepared automated shell script for downloading and unzipping it. Let's use it now and split the dataset stably into train *(0.7)*, validation *(0.2)* and test parts *(0.1)*. Very high download speed (>30MB/s) and connection stability provided by the service should be noted here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "colab_type": "code",
    "id": "j8ESpbziYk61",
    "outputId": "0692e389-3b4c-4735-eaa9-5918c86a0cd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-06-12 17:59:53--  https://www.dropbox.com/s/fi2g3zxsn0pdmn1/nbirds.zip\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.9.1, 2620:100:601f:1::a27d:901\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.9.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucdb7c855b5e2daa601e6fc04f1d.dl.dropboxusercontent.com/cd/0/get/AIs3fSwXnnm-XwD3jSNT1zOkBTlEd9uxC2p1dibzGfnI1f2bOncX8sp5Wzm0rRLEKOE83hfCnRqSfPWbkpi4goRCUvRbbaaGtFCaEmrrrxdcvTRLbx0gdMbbxX6QyAFBi-u8LkN_MWvmMwzEfjn01ec5-2pNXe4A7KCHET24Snpev_qDGSqNsoSP_Ip-W8vkC9s/file [following]\n",
      "--2018-06-12 17:59:53--  https://ucdb7c855b5e2daa601e6fc04f1d.dl.dropboxusercontent.com/cd/0/get/AIs3fSwXnnm-XwD3jSNT1zOkBTlEd9uxC2p1dibzGfnI1f2bOncX8sp5Wzm0rRLEKOE83hfCnRqSfPWbkpi4goRCUvRbbaaGtFCaEmrrrxdcvTRLbx0gdMbbxX6QyAFBi-u8LkN_MWvmMwzEfjn01ec5-2pNXe4A7KCHET24Snpev_qDGSqNsoSP_Ip-W8vkC9s/file\n",
      "Resolving ucdb7c855b5e2daa601e6fc04f1d.dl.dropboxusercontent.com (ucdb7c855b5e2daa601e6fc04f1d.dl.dropboxusercontent.com)... 162.125.9.6, 2620:100:601f:6::a27d:906\n",
      "Connecting to ucdb7c855b5e2daa601e6fc04f1d.dl.dropboxusercontent.com (ucdb7c855b5e2daa601e6fc04f1d.dl.dropboxusercontent.com)|162.125.9.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 623160299 (594M) [application/zip]\n",
      "Saving to: ‘/content/nn-nbirds/data/nbirds.zip’\n",
      "\n",
      "/content/nn-nbirds/ 100%[===================>] 594.29M  37.3MB/s    in 16s     \n",
      "\n",
      "2018-06-12 18:00:10 (36.9 MB/s) - ‘/content/nn-nbirds/data/nbirds.zip’ saved [623160299/623160299]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "! ./data_tools/download.sh\n",
    "\n",
    "# Split the dataset\n",
    "! ./data_tools/split.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHVe8bmASajU"
   },
   "source": [
    "## 2. Database analysis\n",
    "Now we will load our script for metadata loading & dataset preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-I5wUZoxSajW"
   },
   "outputs": [],
   "source": [
    "from preprocess import build_ds_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4ewFs6HISaji",
    "outputId": "4cced959-d064-4134-fb40-1b97eeaeb23c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantity of bounding_boxes: 48562\n",
      "Quantity of sizes: 48562\n",
      "Quantity of hierarchy: 1010\n",
      "Quantity of photographers: 48562\n",
      "Quantity of image_class_labels: 48562\n",
      "Quantity of classes: 1011\n",
      "Quantity of images: 48562\n"
     ]
    }
   ],
   "source": [
    "ds_meta = build_ds_meta()\n",
    "\n",
    "for ds_meta_item in ds_meta:\n",
    "    print('Quantity of %s: %s' % (ds_meta_item, ds_meta[ds_meta_item].count()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uRdGcIQVSajs"
   },
   "source": [
    "The full database contains 48562 images distributed among 1011 classes in total. However the investigated subset consists of only 3010 bird pictures. They are representants of 50 species (classes). The quantity distribution of images in different classes is presented in the Chart 1. We see that it seems to be pretty uniform with 60 items on average.\n",
    "\n",
    "![](http://mion.elka.pw.edu.pl/~mchmielo/tmp/wykres_hist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1wC-6mmJSaju"
   },
   "source": [
    "Since different classes refer to appropriate bird species, which hierarchy is available as database metadata file, we were able to build classes dependency tree similar to the natural species hierarchy. The result graph, in SVG format, can be obtained under the following address [link](http://mion.elka.pw.edu.pl/~mchmielo/tmp/nbirds_hierarchy.svg).\n",
    "\n",
    "Besides hierarchical class assignment database metadata contains some information about authors and dimensions of pictures. Also image bounding boxes are very important from analytical / computational point of view. It contain coordinates of minimal cropping areas which include whole animals. By applying cropping operation we are able to vastly reduce the amount of data send to the classifier (early discarding irrelevant pixels). Image file names are linked with the described metadata by unique *image_guid* properties. The relations between all described files are well presented in the Figure 2.\n",
    "\n",
    "**Figure 2** Database metadata files ![Figure 2](https://raw.githubusercontent.com/zacateras/nn-nbirds/master/assets/dataset.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wqCcg6FBSajw"
   },
   "source": [
    "## 3. Feature extraction with Gabor Filters + Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QoLa3r4JSajy"
   },
   "source": [
    "In order to prepare dataset images library for feature extraction we shall apply a few transformations. All pictures were cropped with bounding boxes defined in metadata, resized to 150 / 150 dimensions, converted to grayscale and transformed with Gabor filter. Hyperparameters of the filter were adjusted, so that different color and contrast bounderies were exposed. New images will be copied to the separate folder.\n",
    "\n",
    "Example images before and after transformation are presented below:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"https://github.com/zacateras/nn-nbirds/blob/master/assets/sample_before.jpg?raw=1\" alt=\"Sample Before\" style=\"height: 300px;\"/></td>\n",
    "        <td><img src=\"https://github.com/zacateras/nn-nbirds/blob/master/assets/sample_after.jpg?raw=1\" alt=\"Sample After\" style=\"height: 300px;\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "We set up stream image data generator to use prepared train folder and randomly apply a set of transforms - rotations and shifts on preprocessed images.\n",
    "\n",
    "Then we initialized a sequence of NN layers. We fixed the number of layers to 2 hidden with Relu activation followed by a dense layer with 50 neurons (matching the number of classes) and sigmoid activation. As we discovered here, Keras is very flexible so that any rearrangements of the model are pretty fast and intuitive for a person with sufficient theoretical NN / DNN knowledge. Its API is also very readable.\n",
    "\n",
    "Since our task was to test hidden layers of different sizes a set of models is created. We tested each combination of 64, 256, 1024 neuron quantities. Generated models were saved directly in a separate folder in the repository. Our best model yielded a bit more than 14% *categorical accuracy* and 39% of *top 5 categorical accuracy*. Detailed performances results are presented in the table below. The network architecture is encoded by appropriate names, following scheme: *d{number_of_neurons_in_the_layer_1}_{layer_1_activation}_d{number_of_neurons_in_the_layer_2}_{layer_2_activation}*.\n",
    "\n",
    "<table>\n",
    "    <img src=\"https://github.com/zacateras/nn-nbirds/blob/master/assets/perceptron_results.png?raw=1\" alt=\"Perceptron Results\" style=\"height: 400px;\"/>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Zgc3Znjc2ye"
   },
   "source": [
    "## 4. Inception v3 \n",
    "For our baseline model, we have chosen Inception v3 model, described here: https://arxiv.org/abs/1512.00567.\n",
    "\n",
    "![alt text](https://cloud.google.com/tpu/docs/images/inceptionv3onc--oview.png)\n",
    "\n",
    "We used pretrained weights available in Keras framework and trained only top layers (visible in the 'Final part'  rectangle in the image above). This allowed us to save a lot of training time (there are about 24 milions of parameters in Inception v3 model and we've trained only two milions of them, which is about 8%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7Uh2TsoldJ62"
   },
   "outputs": [],
   "source": [
    "width = height = 299\n",
    "\n",
    "# First, we import model from keras framework\n",
    "\n",
    "import keras\n",
    "\n",
    "base_model = keras.applications.inception_v3.InceptionV3(include_top=False,\n",
    "                                                         weights='imagenet',\n",
    "                                                         input_shape=(width, height, 3))\n",
    "\n",
    "# Then, we add top layers\n",
    "\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(50, activation='softmax')(x)\n",
    "\n",
    "#We set pretrained layers as non trainable... :\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "model = keras.models.Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy', 'top_k_categorical_accuracy'])\n",
    "\n",
    "\n",
    "# And then we train and evaluate model\n",
    "from preprocess import clip, resize\n",
    "from generators import Generators\n",
    "\n",
    "\n",
    "clip()\n",
    "resize(width, height)\n",
    "\n",
    "g = Generators(width=width, height=height, batch_size=32)\n",
    "\n",
    "(g_train, cnt_train) = g.train()\n",
    "(g_validation, cnt_validation) = g.validation()\n",
    "(g_test, cnt_test) = g.test()\n",
    "\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(filepath='weights-{epoch:02d}.hdf5', verbose=1)\n",
    "\n",
    "model.fit_generator(g_train, validation_data=g_validation, epochs=30, callbacks=[checkpointer])\n",
    "\n",
    "models = sorted([m for m in os.listdir(os.getcwd()) if m.endswith(\"hdf5\")])\n",
    "\n",
    "def eval_model(model):\n",
    "  m = keras.models.load_model(model)\n",
    "  res = m.evaluate_generator(g_test, verbose=True)\n",
    "  del m\n",
    "  keras.backend.clear_session()\n",
    "\n",
    "results = [eval_model(m) for m in models]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xQ4kdvTgACm"
   },
   "source": [
    "We have managed to achieve 68% percent of categorical accuracy and 95% top_5_categorical_accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2HN5qHElSaj0"
   },
   "source": [
    "## 5. Convolutional Neural Network - custom models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXqa9ISCSaj4"
   },
   "source": [
    "Since in the second part of the project we had expected to do a lot more calculations, collect a lot more logs and test a lot more different neural network architectures, it was essential to prepare an efficient way of gather training outputs. We decided to prepare a script which automatically synchronizes a directory tree creating a copy of the results in Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kYobBxVUuorm"
   },
   "outputs": [],
   "source": [
    "! pip install -U -q PyDrive\n",
    "from gdrive import synchronize_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T1xsaGFVSakE"
   },
   "source": [
    "Then we prepared script for executing the whole training process for one network parametrization. We assumed that the following parameters can be parametrized:\n",
    "\n",
    "* maximum number of epochs (epochs),\n",
    "* size of mini-batch (batch_size),\n",
    "* width of input images (width),\n",
    "* height of input images (height),\n",
    "* number of neurons in descriptor layer (descriptor_size),\n",
    "* dimensions of filters used in each convolution layer (filter_size),\n",
    "* number of filters in each convolution layer (filter_number),\n",
    "* regularization method used (kernel_reg - one from None, L1, L2).\n",
    "\n",
    "Each training cycle involves a preprocessing phase: input immage clipping and resizing and a postprocessing phase: collecting logs and saved models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-GgA63WnTiWA",
    "outputId": "2b6ac4a5-3be9-420e-9bc3-a8c52f4fe8a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "from preprocess import clip, resize\n",
    "from generators import Generators\n",
    "\n",
    "from models.custom_dnn import custom_dnn\n",
    "\n",
    "def process_custom(epochs, batch_size, width, height, descriptor_size, filter_size, filter_number, kernel_reg, kernel_reg_name):\n",
    "    model_code = 'cnn_w%s_h%s_%s_descr_%s_x_%s_%s_filt_%s_batch_%s_reg_%s_epochs' % (width, height, descriptor_size, filter_number, filter_size[0], filter_size[1], batch_size, kernel_reg_name, epochs)\n",
    "    model_path = 'cnn_models/%s.h5' % model_code\n",
    "    model_logs_path = 'logs/%s' % model_code\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        print('Model %s already exists! Skipping execution.' % model_code)\n",
    "        return\n",
    "    else:\n",
    "        print('Training model %s...' % model_code)\n",
    "    \n",
    "    if os.path.exists(model_logs_path):\n",
    "        shutil.rmtree(model_logs_path)\n",
    "    os.makedirs(model_logs_path)\n",
    "    os.makedirs('%s/tb' % model_logs_path)\n",
    "    os.makedirs('%s/checkpoints' % model_logs_path) \n",
    "    \n",
    "    # preprocess\n",
    "    clip()\n",
    "    resize(width, height)\n",
    "    \n",
    "    # generators\n",
    "    g = Generators(width=width, height=height, batch_size=batch_size)\n",
    "\n",
    "    (g_train, cnt_train) = g.train()\n",
    "    (g_validation, cnt_validation) = g.validation()\n",
    "    (g_test, cnt_test) = g.test()\n",
    "    \n",
    "    model = custom_dnn(\n",
    "      width=width,\n",
    "      height=height,\n",
    "      output=cnt_train,\n",
    "      descriptor_size=descriptor_size,\n",
    "      filter_size=filter_size,\n",
    "      filter_number=filter_number,\n",
    "      kernel_reg=kernel_reg,\n",
    "      kernel_reg_name=kernel_reg_name)\n",
    "\n",
    "    cb_tb = keras.callbacks.TensorBoard(\n",
    "      log_dir='%s/tb' % model_logs_path,\n",
    "      histogram_freq=0,\n",
    "      batch_size=batch_size,\n",
    "      write_graph=True,\n",
    "      write_grads=False,\n",
    "      write_images=True)\n",
    "\n",
    "    cb_mc = keras.callbacks.ModelCheckpoint(\n",
    "      filepath='%s/checkpoints/weights-{epoch:02d}.hdf5' % model_logs_path,\n",
    "      verbose=1)\n",
    "\n",
    "    model.fit_generator(\n",
    "      g_train,\n",
    "      steps_per_epoch=2000 // batch_size,\n",
    "      epochs=epochs,\n",
    "      validation_data=g_validation,\n",
    "      validation_steps=800 // batch_size,\n",
    "      callbacks=[cb_tb]) # ModelCheckpointer was removed due to significant size of generated checkpoint files\n",
    "\n",
    "    model.save(model_path)\n",
    "    synchronize_all(drive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EiTlVg2JSaka"
   },
   "source": [
    "In order to track changes in performance of the models on-line we exposed a TensorBoard service via localtunnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "cPBVkBwzX19l",
    "outputId": "781f9211-52b5-4e05-c959-330b273d3691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your url is: https://fast-falcon-4.localtunnel.me"
     ]
    }
   ],
   "source": [
    "# Start tensorboard & expose via localtunnel\n",
    "! npm install -g localtunnel > npm-lt.log\n",
    "get_ipython().system_raw('tensorboard --logdir logs --host 0.0.0.0 --port 6006 &')\n",
    "get_ipython().system_raw('lt --port 6006 > url.txt 2>&1 &')\n",
    "! cat url.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wkU44fDRSaky"
   },
   "source": [
    "Then we started the whole training process. We prepared a list of different parametrizations taking a cross-product of three batch sizes *32, 64, 128*, three input image dimensions *64 x 64, 128 x 128, 256 x 256*, three descriptor sizes *256, 512, 1024*, two filter sizes *3 x 3, 5 x 5*, two filter numbers *16, 32*, three types of regularization *None, L1, L2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "colab_type": "code",
    "id": "_b0SzLsVTiWR",
    "outputId": "cdda681b-6484-4d80-e80b-5447666a7d00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model cnn_w128_h128_512_descr_32_x_3_3_filt_128_batch_L2(0.01)_reg_150_epochs...\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import os\n",
    "from itertools import product\n",
    "\n",
    "epochss=[150]\n",
    "batch_sizes=[128, 32, 64]\n",
    "dims=[(64, 64), (128, 128), (256, 256)]\n",
    "\n",
    "descriptor_sizes=[512, 1024, 256]\n",
    "filter_sizes=[(3, 3), (5, 5)]\n",
    "filter_numbers=[32, 16]\n",
    "keras_regs=[(None, None), (keras.regularizers.l1(0.01), 'L1(0.01)'), (keras.regularizers.l2(0.01), 'L2(0.01)')]\n",
    "\n",
    "for item in product(epochss, batch_sizes, dims, descriptor_sizes, filter_sizes, filter_numbers, keras_regs):\n",
    "    epochs = item[0]\n",
    "    batch_size = item[1]\n",
    "    width = item[2][0]\n",
    "    height = item[2][1]\n",
    "    \n",
    "    descriptor_size = item[3]\n",
    "    filter_size = item[4]\n",
    "    filter_number = item[5]\n",
    "    kernel_reg = item[6][0]\n",
    "    kernel_reg_name = item[6][1]\n",
    "    \n",
    "    process_custom(epochs, batch_size, width, height, descriptor_size, filter_size, filter_number, kernel_reg, kernel_reg_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vmxxPgqFSalI"
   },
   "source": [
    "Trained models were also immediately commited to the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "C-5HXuKm7wkR",
    "outputId": "982e4310-1e49-4b08-d8f6-73eaa1f06043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 921e27a] cnn_models\r\n",
      " 1 file changed, 0 insertions(+), 0 deletions(-)\r\n",
      " create mode 100644 cnn_models/cnn_512_descr_32_x_3_3_filt_128_batch_None_reg_120_epochs.h5\n",
      "Counting objects: 4, done.\n",
      "Delta compression using up to 2 threads.\n",
      "Compressing objects: 100% (4/4), done.\n",
      "Writing objects: 100% (4/4), 13.20 MiB | 5.81 MiB/s, done.\n",
      "Total 4 (delta 1), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
      "To https://github.com/zacateras/nn-nbirds.git\n",
      "   c63d25a..921e27a  master -> master\n"
     ]
    }
   ],
   "source": [
    "! git config --global user.email \"USERMAIL\"\n",
    "! git config --global user.name \"USERNAME\"\n",
    "! git add cnn_models/*\n",
    "! git commit -m \"cnn_models\"\n",
    "! git remote set-url origin https://USERNAME:PASSWORD@github.com/zacateras/nn-nbirds.git\n",
    "! git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h1QwYTafSalc"
   },
   "source": [
    "Then we loaded all obtained models and tested them against the training set prepared at the beginning of the project. The results were stored in *cnn_models / results.csv* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "n0-hHpWT54NS",
    "outputId": "2fc995c6-41da-408e-b231-6d883e2d8066"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from generators import Generators\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "records = None\n",
    "\n",
    "for filename in os.listdir('cnn_models'):\n",
    "    if not filename.endswith('.h5'):\n",
    "        continue\n",
    "    \n",
    "    if records is not None and 'filename' in records.columns:\n",
    "        if filename in [item for item in records['filename']]:\n",
    "            print('Skipping for %s...' % filename)\n",
    "            continue\n",
    "        \n",
    "    print('Processing %s...' % filename)\n",
    "\n",
    "    regex = re.compile('cnn_w(\\d+)_h(\\d+)_(\\d+)_descr_(\\d+)_x_(\\d+)_(\\d+)_filt_(\\d+)_batch_(L1\\(0.01\\)|L1\\(0.0001\\)|L2\\(0.01\\)|L2\\(0.0001\\)|None)_reg_(\\d+)_epochs')\n",
    "    match = regex.match(filename)\n",
    "    record = {\n",
    "        \"filename\": filename,\n",
    "        \"width\": int(match.group(1)),\n",
    "        \"height\": int(match.group(2)),\n",
    "        \"descriptor_size\": int(match.group(3)),\n",
    "        \"filter_count\": int(match.group(4)),\n",
    "        \"filter_width\": int(match.group(5)),\n",
    "        \"filter_height\": int(match.group(6)),\n",
    "        \"mini_batch\": int(match.group(7)),\n",
    "        \"regularization\": match.group(8),\n",
    "        \"epochs\": int(match.group(9))\n",
    "    }\n",
    "\n",
    "    [record['loss'], record['categorical_accuracy'], record['top5_categorical_accuracy']] = load_model('cnn_models/%s' % filename) \\\n",
    "        .evaluate_generator(Generators(record['width'], record['height'], record['mini_batch']).test()[0])\n",
    "\n",
    "    if records is None:\n",
    "        records = pd.DataFrame(record, index=[0])\n",
    "    else:\n",
    "        records = records.append(pd.DataFrame(record, index=[0]))\n",
    "\n",
    "df = pd.DataFrame(records).sort_values('categorical_accuracy', ascending=False)\n",
    "df.to_csv('cnn_models/results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ektt2ZriSalu"
   },
   "source": [
    "## 6. Convolutional Neural Network - custom results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "njsSOoMOSalw"
   },
   "source": [
    "In the table below we present top performing models according to the tests we performed. Due to unavailability of Collaboratory in the last phase of our project we were unable to execute all the tests that we scheduled before. In consequence we tested ~75 out of 300 configurations in total. This is also an important notice for our future use. The Collaboratory service is not intended to be used as a background task execution platform. Intensive continous usage can trigger resource limiting policies, significantly reduce the performance of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8dKdWMvsSaly",
    "outputId": "b8245eb7-355f-46e8-9c28-cf91fcc1b4bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>descriptor_size</th>\n",
       "      <th>mini_batch</th>\n",
       "      <th>filter_count</th>\n",
       "      <th>filter_height</th>\n",
       "      <th>filter_width</th>\n",
       "      <th>regularization</th>\n",
       "      <th>categorical_accuracy</th>\n",
       "      <th>top5_categorical_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>0.617792</td>\n",
       "      <td>0.853377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>0.589786</td>\n",
       "      <td>0.841845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>0.574959</td>\n",
       "      <td>0.833608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>0.570016</td>\n",
       "      <td>0.817133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>1024</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>0.566722</td>\n",
       "      <td>0.840198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>0.561779</td>\n",
       "      <td>0.812191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>0.556837</td>\n",
       "      <td>0.838550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>0.551895</td>\n",
       "      <td>0.817133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  width  height  descriptor_size  mini_batch  filter_count  \\\n",
       "0     150    128     128              512         128            32   \n",
       "0     250     64      64              512         128            32   \n",
       "0     150     64      64              512         128            32   \n",
       "0     150     64      64              512         128            32   \n",
       "0     150     64      64             1024          32            32   \n",
       "0     150     64      64              256         128            32   \n",
       "0     120     64      64              512         128            32   \n",
       "0     150     64      64             1024         128            32   \n",
       "\n",
       "   filter_height  filter_width regularization  categorical_accuracy  \\\n",
       "0              3             3           None              0.617792   \n",
       "0              3             3           None              0.589786   \n",
       "0              5             5           None              0.574959   \n",
       "0              3             3           None              0.570016   \n",
       "0              3             3           None              0.566722   \n",
       "0              3             3           None              0.561779   \n",
       "0              3             3           None              0.556837   \n",
       "0              3             3           None              0.551895   \n",
       "\n",
       "   top5_categorical_accuracy  \n",
       "0                   0.853377  \n",
       "0                   0.841845  \n",
       "0                   0.833608  \n",
       "0                   0.817133  \n",
       "0                   0.840198  \n",
       "0                   0.812191  \n",
       "0                   0.838550  \n",
       "0                   0.817133  "
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['categorical_accuracy'] > 0.05]\\\n",
    "    [['epochs', 'width', 'height', 'descriptor_size', 'mini_batch', 'filter_count', 'filter_height', 'filter_width', 'regularization', 'categorical_accuracy', 'top5_categorical_accuracy']]\\\n",
    "    .sort_values(['categorical_accuracy'], ascending=[0])\\\n",
    "    .head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0OH2xCZwgkA"
   },
   "source": [
    "The best model we were able to obtain is the one shown in the picture below. It was trained with images rescaled to 128px x 128px:\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/zacateras/nn-nbirds/master/images/my_cnn.png)\n",
    "\n",
    "In this case we achieved categorical accuracy at the level of **62%** on the test set. The ROC curve (averaged) of the model is presented in the figure below, on the left:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"https://github.com/zacateras/nn-nbirds/blob/master/images/index.png?raw=1\" alt=\"\" style=\"height: 300px;\"/></td>\n",
    "        <td><img src=\"https://github.com/zacateras/nn-nbirds/blob/master/images/index10.png?raw=1\" alt=\"\" style=\"height: 300px;\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Since training of 128 x 128 models took long, we performed the rest of the experiments for nets operating on 64 x 64 input images. The topology the network was not changed. For such images we observed the highest categorical accuracy at the level of **59%** on the test set, when the training was continued until 250s' epoch and **57%** for natural 150 epochs experiments. We assumed this model as the baseline for the next few analytical experiments. The ROC curve of this model is the one above on the right.\n",
    "\n",
    "Changing the size of filters from (3 x 3) to (5 x 5) resulted in increasing the accuracy by **0.5%** up to **57.5%**. Different number of the filters used (16, 16, 32, 32, 32, 32) yielded in worse results - **53%**. The experiment of taking different mini-batch sizes resulted in the conclusion - as we lowered this parameter to 64 and then to 32, the categorical accuracy dropped dramatically. For 32 value used, the networks were mostly unable to fit to data - **~2%** accuracy. We also examined performance of 256, 1024. In comparison to 512 descriptor we observed a slight decrease of performance indicators - **55.2%** for 1024 and **54.9%** for 256.\n",
    "\n",
    "In the last experiment we checked if using L1 and L2 regularizations can prevent our model with overfitting. Surprissingly, applying both regularisers - as they are given in Keras out of the box - with lambda set to either 0.01 or 0.0001 resulted in absolute decrease of model accuracy - **2%**. ROC curves comparison is presented below:\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/zacateras/nn-nbirds/master/images/index78a.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U6eMcxKCck9g"
   },
   "source": [
    "## 7.  Support Vector Machines\n",
    "The third part of second phase of our project was to train Support Vector Machines on descriptors obtained from the selected CNN model. We have used the model - *cnn_w128_h128_512_descr_32_x_3_3_filt_128_batch_None_reg_150_epochs*, with baseline accuracy of 61%.\n",
    "\n",
    "SVMs were implemented using Scikit Learn framework, with exception to exponential kernel function. Since it is not as popular as similar Gaussian function, it is not implemented in Sckit Learn yet. Instead we used implementation found here: https://github.com/gmum/pykernels (code from this repository was pulled to our solution and then just slightly adjusted).\n",
    "\n",
    "Using two different SVMs - with polynomial and exponential kernel we have manged to achieve accordingly accuracy of 58% and 64%. We measured an impact of the penalty parameter (C) on classification results. Our measurements are depicted below:\n",
    "\n",
    "For exponential kernel the best results was observed using sigma parameter equal to 7.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./assets/plt_exp.png\" alt=\"\" style=\"height: 300px;\"/></td>\n",
    "        <td><img src=\"./assets/plt_poly.png\" alt=\"\" style=\"height: 300px;\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The penalty parameter balances tradeoff between number of missclassified samples versus simplicity of decision surface. Low C parameter favours simpler boundaries, and high allows to fit them tightly to training data. Intuitivly, too high values of C can result in overfitting. However, for our results, overfitting cannot be observed, what may mean that the data obtained from CNN descriptors are easily linearly separable in transformed feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EYYgW-lTgyRs"
   },
   "source": [
    "## 8. Tools evaluation\n",
    "\n",
    "### Collab\n",
    "\n",
    "Google Collab is very useful for creating and testing small neural network achitectures. It provides GPU for free, which allowed us to train new models in time and cont efficient manner. On the other hand, it has some serious drawbacks, which are unacceptable in professional work. The key shortcomings of Collab are:\n",
    "\n",
    "- instability: getting a decent runtime with GPU sometimes needs a few restarts, sometimes it boots with python 3 instead of 2, or using GPU with only 256 MB of internal memory,\n",
    "- problems with data synchronization: Collab is cloud based and its storage isn't prsistent, and getting data out of it requires some work, i.e. setting up synchronization with Google drive, instead of just copying files with using SCP\n",
    "\n",
    "### Keras\n",
    "\n",
    "Using Keras for network implementation is a pleasure. Popular neural network achitectures with pretrained weights are available out of the box. Thanks to Tensorflow backend training is fast, in comparison to market competitors. The only problem with Keras is relative immaturity. Many branches of the software are insufficiently documented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QI4J8TK4onDC"
   },
   "source": [
    "## 9. Conclusions\n",
    "\n",
    "For image classification problem, Convolutional Neural Networks are far more effective than perceptrons. Using pretrained state of the art model one can achive satysfying results with very short network training. In some cases, CNN results can be further boosted by treating them as descriptor generators and using Support Vector Machines to classify descriptors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NeQw6BOBSal4"
   },
   "source": [
    "## References\n",
    "\n",
    "#### Google Colab\n",
    "* [Colab Teaser](https://hackernoon.com/train-your-machine-learning-models-on-googles-gpus-for-free-forever-a41bd309d6ad)\n",
    "* [Simple Colab Example](https://towardsdatascience.com/fast-ai-lesson-1-on-google-colab-free-gpu-d2af89f53604)\n",
    "* [Extensive Colab Example](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d)\n",
    "\n",
    "#### Gabor Filters\n",
    "* [Practical overview of Gabor Filters](https://cvtuts.wordpress.com/2014/04/27/gabor-filters-a-practical-overview/)\n",
    "* [Library for testing various filter configurations](http://nbviewer.jupyter.org/github/bicv/LogGabor/blob/master/LogGabor.ipynb)\n",
    "* [Example of applying OpenCV implementation in Python](https://gist.github.com/kendricktan/93f0da88d0b25087d751ed2244cf770c)\n",
    "* [Feature extraction in face recognition task](https://pdfs.semanticscholar.org/3cf9/e4bc906ee4d15b69b34db413f8e319692e3b.pdf)\n",
    "* [Feature extraction in car recognition task](http://scholar.google.pl/scholar_url?url=http://www.academia.edu/download/3448045/Car_recognition_using_gabor_filter_feature_extraction.doc&hl=pl&sa=X&scisig=AAGBfm2C3mPP89cH7bXDsFiXMDNPLIY27Q&nossl=1&oi=scholarr&ved=0ahUKEwizv5bdgdTaAhXEGuwKHXy-AY0QgAMIKSgCMAA)\n",
    "\n",
    "#### DNNs in Keras\n",
    "* [Keras documentation](https://keras.io/)\n",
    "* [Keras example](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "nn_nbirds_report.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
